{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4cc1cfd-081c-4666-99e8-93e8757ae4a8",
   "metadata": {},
   "source": [
    "# Transformer test (translation)\n",
    "\n",
    "Reference: https://pytorch.org/tutorials/beginner/translation_transformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "007dbd6a-212c-41dc-8784-0c3f6700ca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbd46454-4f48-4918-8152-fa1f46375401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3eafe6e-1bf7-4ab5-a40a-62ae306ec602",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f52b193-d24f-43c6-a9f2-54bb7ae8a601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc1ca3ad-822d-4461-b531-9c79791bce35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"DEVICE: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55be9111-ab2b-4e77-b21e-4159f8bdd945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import SpecialSymbols, sequential_transforms, tensor_transform, token_to_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8763a82-152f-486b-99c3-82377f2329d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import Transformer as TestTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0824480-a045-4411-a8ae-0339bd24663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a8d0827-e789-4857-bb0e-22cfb2bea4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5043eaa9-98c2-4977-8ebf-32893699c7cf",
   "metadata": {},
   "source": [
    "## Token transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1e4f9e9-4117-4ecd-8b29-5cf236edb7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5444ee8b-072f-43a4-9547-a055deb56096",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23b6550a-5aba-4ef8-a6e7-16dea3bbd58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample src token transform: ['hello', 'world']\n",
      "sample tgt token transform: ['hello', 'world']\n"
     ]
    }
   ],
   "source": [
    "print(f'sample src token transform: {token_transform[SRC_LANGUAGE](\"hello world\")}')\n",
    "print(f'sample tgt token transform: {token_transform[TGT_LANGUAGE](\"hello world\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6851bdb3-ffaa-4bc2-8970-1c536975d844",
   "metadata": {},
   "source": [
    "## Vocab transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64cbb79f-5475-4722-8624-50216639eb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to yield list of tokens\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d44174a0-a91e-4e4e-9526-7f54f4f1d26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # Create torchtext's Vocab object\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=SpecialSymbols.special_symbols,\n",
    "                                                    special_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d41442ca-5db5-4f04-9491-ca4ae85452ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "  vocab_transform[ln].set_default_index(SpecialSymbols.UNK_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380f89d4-3101-4eee-a348-5f6350d126f8",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5505ea5-3fd7-4bb2-a464-19f1ec08023f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7efd9faef330>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3bc24ce6-82b7-4f8c-9b15-c304e4aa3acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC_VOCAB_SIZE: 19214, TGT_VOCAB_SIZE: 10837\n",
      "sample src vocab transform: 0 18975\n",
      "sample src vocab transform: ['<unk>', 'world']\n",
      "sample tgt vocab transform: 5465 1870\n",
      "sample tgt vocab transform: ['hello', 'world']\n"
     ]
    }
   ],
   "source": [
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "print(f\"SRC_VOCAB_SIZE: {SRC_VOCAB_SIZE}, TGT_VOCAB_SIZE: {TGT_VOCAB_SIZE}\")\n",
    "print(f'sample src vocab transform: {vocab_transform[SRC_LANGUAGE][\"hello\"]} {vocab_transform[SRC_LANGUAGE][\"world\"]}')\n",
    "print(f'sample src vocab transform: {vocab_transform[SRC_LANGUAGE].lookup_tokens((0, 18975))}')\n",
    "print(f'sample tgt vocab transform: {vocab_transform[TGT_LANGUAGE][\"hello\"]} {vocab_transform[TGT_LANGUAGE][\"world\"]}')\n",
    "print(f'sample tgt vocab transform: {vocab_transform[TGT_LANGUAGE].lookup_tokens((5465, 1870))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d498f988-2b24-4a82-ada3-be96472b3adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab66d6f1-d7a1-4e1b-b9a2-1cff7aa5ef07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer: Transformer(\n",
      "  (encoder): Encoder(\n",
      "    (positional_embedding): PositionalEmbedding(\n",
      "      (embedding_layer): Embedding(19214, 512)\n",
      "      (position_embedding_layer): PositionEncoding()\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (encoder_layers): ModuleList(\n",
      "      (0-2): 3 x EncoderLayer(\n",
      "        (attention): GlobalSelfAttention(\n",
      "          (multi_head_attention): MultiHeadAttention(\n",
      "            (q_linear_projection_func): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (k_linear_projection_func): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (v_linear_projection_func): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (attention_projection_func): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (attention): ScaledDotProductAttention()\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ff): FeedForwardLayer(\n",
      "          (ff): PositionWiseFeedForward(\n",
      "            (linear_1_func): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (relu_func): ReLU()\n",
      "            (linear_2_func): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (ff_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (ff_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (positional_embedding): PositionalEmbedding(\n",
      "      (embedding_layer): Embedding(19214, 512)\n",
      "      (position_embedding_layer): PositionEncoding()\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (decoder_layers): ModuleList(\n",
      "      (0-2): 3 x DecoderLayer(\n",
      "        (causal_self_attention): CausalSelfAttention(\n",
      "          (multi_head_attention): MultiHeadAttention(\n",
      "            (q_linear_projection_func): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (k_linear_projection_func): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (v_linear_projection_func): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (attention_projection_func): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (attention): ScaledDotProductAttention()\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (cross_attention): CrossAttention(\n",
      "          (multi_head_attention): MultiHeadAttention(\n",
      "            (q_linear_projection_func): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (k_linear_projection_func): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (v_linear_projection_func): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (attention_projection_func): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (attention): ScaledDotProductAttention()\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (ff): FeedForwardLayer(\n",
      "          (ff): PositionWiseFeedForward(\n",
      "            (linear_1_func): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (relu_func): ReLU()\n",
      "            (linear_2_func): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (ff_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (ff_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_layer): Linear(in_features=512, out_features=10837, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "transformer = TestTransformer(\n",
    "    SRC_VOCAB_SIZE,\n",
    "    TGT_VOCAB_SIZE,\n",
    "    EMB_SIZE,\n",
    "    NHEAD,\n",
    "    FFN_HID_DIM,\n",
    "    0.1,\n",
    "    NUM_ENCODER_LAYERS,\n",
    "    NUM_DECODER_LAYERS\n",
    ")\n",
    "print(f\"transformer: {transformer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b845b4fc-1380-468f-8841-d1535b1833ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "252e1b42-c21a-4c76-8585-23727213ef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transformer.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "856e8d0f-2d91-4a40-9c30-6905cfeb3a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=SpecialSymbols.PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "369a5266-43dd-416e-9f94-e897674e847c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manman/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eae56399-8317-4170-9475-ada010cd6aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9834c361-8072-4de5-a0d0-d21575c4fcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to collate data samples into batch tensors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=SpecialSymbols.PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=SpecialSymbols.PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c302ac31-25c1-4f10-a26f-9dd63a457649",
   "metadata": {},
   "source": [
    "## Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f5de3df-11c0-4987-8268-1816f53b8fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.size(): torch.Size([35, 128]), tgt.size(): torch.Size([30, 128])\n",
      "\n",
      "sample sentence 0:\n",
      "    src (de):  Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen                         \n",
      "    tgt (en):  A group of men are loading cotton onto a truck                   \n",
      "\n",
      "\n",
      "sample sentence 1:\n",
      "    src (de):  Ein Mann schläft in einem grünen Raum auf einem Sofa .                       \n",
      "    tgt (en):  A man sleeping in a green room on a couch .                  \n",
      "\n",
      "\n",
      "sample sentence 2:\n",
      "    src (de):  Ein Junge mit Kopfhörern sitzt auf den Schultern einer Frau .                       \n",
      "    tgt (en):  A boy wearing headphones sits on a woman 's shoulders .                  \n",
      "\n",
      "\n",
      "sample sentence 3:\n",
      "    src (de):  Zwei Männer bauen eine blaue <unk> auf einem <unk> See auf                       \n",
      "    tgt (en):  Two men setting up a blue ice fishing hut on an iced over lake               \n",
      "\n",
      "\n",
      "sample sentence 4:\n",
      "    src (de):  Ein Mann mit beginnender Glatze , der eine rote Rettungsweste trägt , sitzt in einem kleinen Boot .                \n",
      "    tgt (en):  A balding man wearing a red life jacket is sitting in a small boat .              \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "sample_val_dataloader = DataLoader(sample_val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "sample_data_batch = next(iter(sample_val_dataloader))\n",
    "# data: (src, tgt)\n",
    "# src/tgt: (tokens, batch_size)\n",
    "print(f\"src.size(): {sample_data_batch[0].size()}, tgt.size(): {sample_data_batch[1].size()}\")\n",
    "sample_print_size = 5\n",
    "for i in range(sample_print_size):\n",
    "    if i >= BATCH_SIZE:\n",
    "        break\n",
    "    sample_src_sentence = token_to_sentence(sample_data_batch[0][:, i].numpy(), vocab_transform[SRC_LANGUAGE], True)\n",
    "    sample_tgt_sentence = token_to_sentence(sample_data_batch[1][:, i].numpy(), vocab_transform[TGT_LANGUAGE], True)\n",
    "    print(f\"\"\"\n",
    "sample sentence {i}:\n",
    "    src ({SRC_LANGUAGE}): {sample_src_sentence}\n",
    "    tgt ({TGT_LANGUAGE}): {sample_tgt_sentence}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded6ac86-206a-4ce8-90b9-12d44cc64a10",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46cfb892-e1d5-41b2-a73a-ddcfe7e9d158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0cc51eb5-6b19-4c7a-8832-133048bf951a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, src):\n",
    "    model.eval()\n",
    "    src = src.to(DEVICE)\n",
    "    tgt = model(src, torch.tensor([[SpecialSymbols.BOS_IDX]]))\n",
    "    return tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b880f9b-4a5e-42a4-aa54-50d9da714f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_input_as_batch: torch.Size([1, 35]), sample_next_token_prob: torch.Size([1, 1, 10837])\n",
      "SRC_VOCAB_SIZE: 19214, TGT_VOCAB_SIZE: 10837\n",
      "sample_next_token_idx: tensor([[5778]]) (torch.Size([1, 1])), sample_next_token_max_prob: tensor([[1.2038]], grad_fn=<MaxBackward0>)\n",
      "sample_next_token: previously\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Man/Documents/Projects/transformer/scaled_dot_product_attention.py:31: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.softmax(output)\n",
      "/tmp/ipykernel_331/82084527.py:9: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  sample_next_token = token_to_sentence(np.array([sample_next_token_idx]), vocab_transform[TGT_LANGUAGE], True)\n"
     ]
    }
   ],
   "source": [
    "sample_input = sample_data_batch[0][:, 0]\n",
    "sample_input_as_batch = sample_data_batch[0][:, 0].unsqueeze(0)\n",
    "sample_next_token_prob = eval(transformer, sample_input_as_batch)\n",
    "print(f\"sample_input_as_batch: {sample_input_as_batch.size()}, sample_next_token_prob: {sample_next_token_prob.size()}\")\n",
    "print(f\"SRC_VOCAB_SIZE: {SRC_VOCAB_SIZE}, TGT_VOCAB_SIZE: {TGT_VOCAB_SIZE}\")\n",
    "\n",
    "sample_next_token_max_prob, sample_next_token_idx = torch.max(sample_next_token_prob, dim=2)\n",
    "print(f\"sample_next_token_idx: {sample_next_token_idx} ({sample_next_token_idx.size()}), sample_next_token_max_prob: {sample_next_token_max_prob}\")\n",
    "sample_next_token = token_to_sentence(np.array([sample_next_token_idx]), vocab_transform[TGT_LANGUAGE], True)\n",
    "print(f\"sample_next_token: {sample_next_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5fde2e-f1be-4d73-b6f7-d719502f3f21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
